
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7d088c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import datetime\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752d58a1",
   "metadata": {},
   "source": [
    "Key improvements to the script:\n",
    "\n",
    "1.Added pagination to fetch multiple pages until we reach 100 titles\n",
    "2.The script now loops through pages (using the ?p=N parameter in the URL)\n",
    "3.Added a 1-second delay between requests to be respectful to the server\n",
    "4.Progress messages to show which page is being fetched\n",
    "5.A check to stop if we reach the end of available stories\n",
    "\n",
    "The script will now collect titles across multiple pages until it reaches 100 titles or runs out of content. It prints progress updates so you can see what's happening as it works.\n",
    "\n",
    "You can adjust the number of titles by changing the num_titles variable in the main() function if you ever need a different amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6037f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_hackernews(num_titles=100):\n",
    "    titles = []\n",
    "    page = 1\n",
    "    \n",
    "    # Keep fetching pages until we have enough titles or run out of pages\n",
    "    while len(titles) < num_titles:\n",
    "        # URL of Hacker News - first page has no parameter, subsequent pages use p=N\n",
    "        url = \"https://news.ycombinator.com/\" if page == 1 else f\"https://news.ycombinator.com/news?p={page}\"\n",
    "        \n",
    "        print(f\"Fetching page {page}...\")\n",
    "        \n",
    "        # Send a GET request to the URL\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to retrieve the webpage: Status code {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find all story titles (they are in <span class=\"titleline\"> elements)\n",
    "        title_elements = soup.find_all('span', class_='titleline')\n",
    "        \n",
    "        # If no more titles found, break the loop\n",
    "        if not title_elements:\n",
    "            print(\"No more titles found.\")\n",
    "            break\n",
    "        \n",
    "        # Extract the titles from this page\n",
    "        for title_element in title_elements:\n",
    "            # The title is in the first a tag inside the span\n",
    "            a_tag = title_element.find('a')\n",
    "            if a_tag:\n",
    "                titles.append(a_tag.text)\n",
    "                # Break if we've reached the desired number of titles\n",
    "                if len(titles) >= num_titles:\n",
    "                    break\n",
    "        \n",
    "        # Move to the next page\n",
    "        page += 1\n",
    "        \n",
    "        # Be nice to the server - add a short delay between requests\n",
    "        time.sleep(1)\n",
    "    \n",
    "    return titles[:num_titles]  # Ensure we return exactly the number requested\n",
    "\n",
    "def save_to_csv(titles):\n",
    "    # Generate a filename with current date and time\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"hackernews_titles_{timestamp}.csv\"\n",
    "    \n",
    "    # Write titles to CSV file\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['Title'])  # Header row\n",
    "        for title in titles:\n",
    "            writer.writerow([title])\n",
    "    \n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f0e717c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping 100 Hacker News titles...\n",
      "Fetching page 1...\n",
      "Fetching page 2...\n",
      "Fetching page 3...\n",
      "Fetching page 4...\n",
      "Found 100 titles.\n",
      "Titles saved to hackernews_titles_20250407_104704.csv\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    num_titles = 100\n",
    "    print(f\"Scraping {num_titles} Hacker News titles...\")\n",
    "    \n",
    "    titles = scrape_hackernews(num_titles)\n",
    "    \n",
    "    if titles:\n",
    "        print(f\"Found {len(titles)} titles.\")\n",
    "        filename = save_to_csv(titles)\n",
    "        print(f\"Titles saved to {filename}\")\n",
    "    else:\n",
    "        print(\"No titles were found or there was an error.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97bfc1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('hackernews_titles_20250407_104704.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e47e460d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rsync replaced with openrsync on macOS Sequoia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AI masters Minecraft: DeepMind program finds d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Glamorous Toolkit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dark Mirror Ideologies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We asked camera companies why their RAW format...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Database Protocols Are Underwhelming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Serving Vector Tiles, Fast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Federal cuts disrupt repairs to iconic U.S. tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Emulating an iPhone in QEMU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Show HN: Clawtype v2.1 – a one-hand chorded US...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title\n",
       "0      Rsync replaced with openrsync on macOS Sequoia\n",
       "1   AI masters Minecraft: DeepMind program finds d...\n",
       "2                                   Glamorous Toolkit\n",
       "3                              Dark Mirror Ideologies\n",
       "4   We asked camera companies why their RAW format...\n",
       "..                                                ...\n",
       "95               Database Protocols Are Underwhelming\n",
       "96                         Serving Vector Tiles, Fast\n",
       "97  Federal cuts disrupt repairs to iconic U.S. tr...\n",
       "98                        Emulating an iPhone in QEMU\n",
       "99  Show HN: Clawtype v2.1 – a one-hand chorded US...\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7aa37e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
