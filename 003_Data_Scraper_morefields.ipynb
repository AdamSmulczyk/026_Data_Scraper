


{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f0b1519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import datetime\n",
    "import time\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3debca69",
   "metadata": {},
   "source": [
    "This enhanced script now:\n",
    "\n",
    "1.Collects comprehensive information about each post including:\n",
    "\n",
    "Title\n",
    "URL\n",
    "Points/score (upvotes)\n",
    "Author username\n",
    "Age of the post (e.g., \"1 hour ago\")\n",
    "Number of comments\n",
    "\n",
    "\n",
    "2.Uses regular expressions to extract numeric values from text (like \"123 points\" â†’ 123)\n",
    "3.Properly handles the Hacker News HTML structure where each story has multiple rows\n",
    "4.Saves all this information to a CSV file with appropriate column headers\n",
    "5.Maintains the pagination functionality to collect up to 100 articles\n",
    "6.Includes error handling for posts that might be missing certain fields\n",
    "\n",
    "When you run this script, you'll get a much more comprehensive dataset about the articles on Hacker News. The filename will include a timestamp so you can track when the data was collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ca81961",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_hackernews(num_titles=100):\n",
    "    articles = []\n",
    "    page = 1\n",
    "    \n",
    "    # Keep fetching pages until we have enough titles or run out of pages\n",
    "    while len(articles) < num_titles:\n",
    "        # URL of Hacker News - first page has no parameter, subsequent pages use p=N\n",
    "        url = \"https://news.ycombinator.com/\" if page == 1 else f\"https://news.ycombinator.com/news?p={page}\"\n",
    "        \n",
    "        print(f\"Fetching page {page}...\")\n",
    "        \n",
    "        # Send a GET request to the URL\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to retrieve the webpage: Status code {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find all story rows (each story has 3 rows: title row, spacer row, and info row)\n",
    "        story_rows = soup.find_all('tr', class_='athing')\n",
    "        \n",
    "        # If no more stories found, break the loop\n",
    "        if not story_rows:\n",
    "            print(\"No more articles found.\")\n",
    "            break\n",
    "        \n",
    "        # Process each story\n",
    "        for story_row in story_rows:\n",
    "            # Get the story ID\n",
    "            story_id = story_row.get('id')\n",
    "            \n",
    "            # Get the title and URL\n",
    "            title_span = story_row.find('span', class_='titleline')\n",
    "            title = \"\"\n",
    "            url = \"\"\n",
    "            if title_span:\n",
    "                title_link = title_span.find('a')\n",
    "                if title_link:\n",
    "                    title = title_link.text\n",
    "                    url = title_link.get('href', '')\n",
    "            \n",
    "            # Find the subtext row which contains points, author, time, and comments\n",
    "            subtext_row = soup.find('td', class_='subtext', id=f\"{story_id}_meta\")\n",
    "            \n",
    "            # Initialize variables with default values\n",
    "            points = 0\n",
    "            author = \"\"\n",
    "            age = \"\"\n",
    "            comments = 0\n",
    "            \n",
    "            if subtext_row:\n",
    "                # Get points\n",
    "                score_span = subtext_row.find('span', class_='score')\n",
    "                if score_span:\n",
    "                    # Extract the number from \"123 points\"\n",
    "                    points_text = score_span.text\n",
    "                    points_match = re.search(r'(\\d+)', points_text)\n",
    "                    if points_match:\n",
    "                        points = int(points_match.group(1))\n",
    "                \n",
    "                # Get author\n",
    "                user_link = subtext_row.find('a', class_='hnuser')\n",
    "                if user_link:\n",
    "                    author = user_link.text\n",
    "                \n",
    "                # Get age\n",
    "                age_span = subtext_row.find('span', class_='age')\n",
    "                if age_span:\n",
    "                    age = age_span.text\n",
    "                \n",
    "                # Get comment count\n",
    "                comment_links = subtext_row.find_all('a')\n",
    "                for link in comment_links:\n",
    "                    if 'comment' in link.text or 'discuss' in link.text:\n",
    "                        # Extract the number from \"123 comments\" or default to 0 for \"discuss\"\n",
    "                        comment_text = link.text\n",
    "                        comment_match = re.search(r'(\\d+)', comment_text)\n",
    "                        if comment_match:\n",
    "                            comments = int(comment_match.group(1))\n",
    "                        break\n",
    "            \n",
    "            # Add the article data to our list\n",
    "            article_data = {\n",
    "                'title': title,\n",
    "                'url': url,\n",
    "                'points': points,\n",
    "                'author': author,\n",
    "                'age': age,\n",
    "                'comments': comments\n",
    "            }\n",
    "            \n",
    "            articles.append(article_data)\n",
    "            \n",
    "            # Break if we've reached the desired number of articles\n",
    "            if len(articles) >= num_titles:\n",
    "                break\n",
    "        \n",
    "        # Move to the next page\n",
    "        page += 1\n",
    "        \n",
    "        # Be nice to the server - add a short delay between requests\n",
    "        time.sleep(1)\n",
    "    \n",
    "    return articles[:num_titles]  # Ensure we return exactly the number requested\n",
    "\n",
    "def save_to_csv(articles):\n",
    "    # Generate a filename with current date and time\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"hackernews_articles_{timestamp}.csv\"\n",
    "    \n",
    "    # Write articles to CSV file\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        # Define fields for the CSV\n",
    "        fieldnames = ['title', 'url', 'points', 'author', 'age', 'comments']\n",
    "        \n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()  # Write header row\n",
    "        \n",
    "        for article in articles:\n",
    "            writer.writerow(article)\n",
    "    \n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5610419e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping 100 Hacker News articles...\n",
      "Fetching page 1...\n",
      "Fetching page 2...\n",
      "Fetching page 3...\n",
      "Fetching page 4...\n",
      "Found 100 articles.\n",
      "Articles saved to hackernews_articles_20250407_105948.csv\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    num_articles = 100\n",
    "    print(f\"Scraping {num_articles} Hacker News articles...\")\n",
    "    \n",
    "    articles = scrape_hackernews(num_articles)\n",
    "    \n",
    "    if articles:\n",
    "        print(f\"Found {len(articles)} articles.\")\n",
    "        filename = save_to_csv(articles)\n",
    "        print(f\"Articles saved to {filename}\")\n",
    "    else:\n",
    "        print(\"No articles were found or there was an error.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76823e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('hackernews_articles_20250407_105948.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99b30de0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>points</th>\n",
       "      <th>author</th>\n",
       "      <th>age</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rsync replaced with openrsync on macOS Sequoia</td>\n",
       "      <td>https://derflounder.wordpress.com/2025/04/06/r...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AI masters Minecraft: DeepMind program finds d...</td>\n",
       "      <td>https://www.nature.com/articles/d41586-025-010...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Glamorous Toolkit</td>\n",
       "      <td>https://gtoolkit.com//</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dark Mirror Ideologies</td>\n",
       "      <td>https://www.fortressofdoors.com/dark-mirror-id...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We asked camera companies why their RAW format...</td>\n",
       "      <td>https://www.theverge.com/tech/640119/camera-ra...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Database Protocols Are Underwhelming</td>\n",
       "      <td>https://byroot.github.io/performance/2025/03/2...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>For the first time in 25 years, California has...</td>\n",
       "      <td>https://www.latimes.com/environment/story/2025...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Serving Vector Tiles, Fast</td>\n",
       "      <td>https://spatialists.ch/posts/2025/04-05-servin...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Emulating an iPhone in QEMU</td>\n",
       "      <td>https://eshard.com/posts/emulating-ios-14-with...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Show HN: Clawtype v2.1 â€“ a one-hand chorded US...</td>\n",
       "      <td>https://www.youtube.com/watch?v=N2PSiOl-auM</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0      Rsync replaced with openrsync on macOS Sequoia   \n",
       "1   AI masters Minecraft: DeepMind program finds d...   \n",
       "2                                   Glamorous Toolkit   \n",
       "3                              Dark Mirror Ideologies   \n",
       "4   We asked camera companies why their RAW format...   \n",
       "..                                                ...   \n",
       "95               Database Protocols Are Underwhelming   \n",
       "96  For the first time in 25 years, California has...   \n",
       "97                         Serving Vector Tiles, Fast   \n",
       "98                        Emulating an iPhone in QEMU   \n",
       "99  Show HN: Clawtype v2.1 â€“ a one-hand chorded US...   \n",
       "\n",
       "                                                  url  points  author  age  \\\n",
       "0   https://derflounder.wordpress.com/2025/04/06/r...       0     NaN  NaN   \n",
       "1   https://www.nature.com/articles/d41586-025-010...       0     NaN  NaN   \n",
       "2                              https://gtoolkit.com//       0     NaN  NaN   \n",
       "3   https://www.fortressofdoors.com/dark-mirror-id...       0     NaN  NaN   \n",
       "4   https://www.theverge.com/tech/640119/camera-ra...       0     NaN  NaN   \n",
       "..                                                ...     ...     ...  ...   \n",
       "95  https://byroot.github.io/performance/2025/03/2...       0     NaN  NaN   \n",
       "96  https://www.latimes.com/environment/story/2025...       0     NaN  NaN   \n",
       "97  https://spatialists.ch/posts/2025/04-05-servin...       0     NaN  NaN   \n",
       "98  https://eshard.com/posts/emulating-ios-14-with...       0     NaN  NaN   \n",
       "99        https://www.youtube.com/watch?v=N2PSiOl-auM       0     NaN  NaN   \n",
       "\n",
       "    comments  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          0  \n",
       "4          0  \n",
       "..       ...  \n",
       "95         0  \n",
       "96         0  \n",
       "97         0  \n",
       "98         0  \n",
       "99         0  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1d2ed02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 6 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   title     100 non-null    object \n",
      " 1   url       100 non-null    object \n",
      " 2   points    100 non-null    int64  \n",
      " 3   author    0 non-null      float64\n",
      " 4   age       0 non-null      float64\n",
      " 5   comments  100 non-null    int64  \n",
      "dtypes: float64(2), int64(2), object(2)\n",
      "memory usage: 4.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63c4742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c919088",
   "metadata": {},
   "source": [
    "The main issue was with how I was trying to find the subtext row that contains the metadata. Here are the key fixes:\n",
    "\n",
    "1.Fixed the subtext row selection:\n",
    "\n",
    "-The previous method was trying to find subtext by ID, which doesn't work\n",
    "-Now I'm using find_next_sibling('tr') to get the row that follows each story row\n",
    "-Then finding the <td class=\"subtext\"> within that row\n",
    "\n",
    "\n",
    "2.Added proper data cleaning:\n",
    "\n",
    "-Added .strip() to remove any extra whitespace from text fields\n",
    "\n",
    "\n",
    "3.Improved debugging:\n",
    "\n",
    "-Added a preview of the first 5 articles at the end of the script so you can verify the data is being collected properly\n",
    "\n",
    "\n",
    "4.Fixed the comment count extraction:\n",
    "\n",
    "-Improved the regex pattern matching for finding comment counts\n",
    "\n",
    "\n",
    "The script should now correctly populate all columns in the CSV file, including points, author, age, and comment counts. The preview at the end will help you verify that the data is being collected correctly before you check the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "975586b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping 100 Hacker News articles...\n",
      "Fetching page 1...\n",
      "Fetching page 2...\n",
      "Fetching page 3...\n",
      "Fetching page 4...\n",
      "Found 100 articles.\n",
      "Articles saved to hackernews_articles_20250407_110949.csv\n",
      "\n",
      "Preview of the first 5 articles:\n",
      "\n",
      "Article 1:\n",
      "  title: Rsync replaced with openrsync on macOS Sequoia\n",
      "  url: https://derflounder.wordpress.com/2025/04/06/rsync-replaced-with-openrsync-on-macos-sequoia/\n",
      "  points: 330\n",
      "  author: zdw\n",
      "  age: 11 hours ago\n",
      "  comments: 255\n",
      "\n",
      "Article 2:\n",
      "  title: AI masters Minecraft: DeepMind program finds diamonds without being taught\n",
      "  url: https://www.nature.com/articles/d41586-025-01019-w\n",
      "  points: 61\n",
      "  author: LinuxBender\n",
      "  age: 5 hours ago\n",
      "  comments: 30\n",
      "\n",
      "Article 3:\n",
      "  title: Glamorous Toolkit\n",
      "  url: https://gtoolkit.com//\n",
      "  points: 152\n",
      "  author: radeeyate\n",
      "  age: 9 hours ago\n",
      "  comments: 33\n",
      "\n",
      "Article 4:\n",
      "  title: We asked camera companies why their RAW formats are all different and confusing\n",
      "  url: https://www.theverge.com/tech/640119/camera-raw-spec-format-explained-adobe-dng-canon-nikon-sony-fujifilm\n",
      "  points: 113\n",
      "  author: Tomte\n",
      "  age: 8 hours ago\n",
      "  comments: 36\n",
      "\n",
      "Article 5:\n",
      "  title: Writing C for Curl\n",
      "  url: https://daniel.haxx.se/blog/2025/04/07/writing-c-for-curl/\n",
      "  points: 46\n",
      "  author: TangerineDream\n",
      "  age: 2 hours ago\n",
      "  comments: 4\n"
     ]
    }
   ],
   "source": [
    "def scrape_hackernews(num_titles=100):\n",
    "    articles = []\n",
    "    page = 1\n",
    "    \n",
    "    # Keep fetching pages until we have enough titles or run out of pages\n",
    "    while len(articles) < num_titles:\n",
    "        # URL of Hacker News - first page has no parameter, subsequent pages use p=N\n",
    "        url = \"https://news.ycombinator.com/\" if page == 1 else f\"https://news.ycombinator.com/news?p={page}\"\n",
    "        \n",
    "        print(f\"Fetching page {page}...\")\n",
    "        \n",
    "        # Send a GET request to the URL\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to retrieve the webpage: Status code {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find all story rows\n",
    "        story_rows = soup.find_all('tr', class_='athing')\n",
    "        \n",
    "        # If no more stories found, break the loop\n",
    "        if not story_rows:\n",
    "            print(\"No more articles found.\")\n",
    "            break\n",
    "        \n",
    "        # Process each story\n",
    "        for story_row in story_rows:\n",
    "            story_id = story_row.get('id')\n",
    "            \n",
    "            # Get the title and URL\n",
    "            title_span = story_row.find('span', class_='titleline')\n",
    "            title = \"\"\n",
    "            url = \"\"\n",
    "            if title_span:\n",
    "                title_link = title_span.find('a')\n",
    "                if title_link:\n",
    "                    title = title_link.text.strip()\n",
    "                    url = title_link.get('href', '').strip()\n",
    "            \n",
    "            # FIXED: Look for the subtext in the next row\n",
    "            # The subtext is in the next tr after the athing tr\n",
    "            subtext_row = story_row.find_next_sibling('tr')\n",
    "            \n",
    "            # Initialize variables with default values\n",
    "            points = 0\n",
    "            author = \"\"\n",
    "            age = \"\"\n",
    "            comments = 0\n",
    "            \n",
    "            if subtext_row:\n",
    "                # The subtext is in a td with class=\"subtext\"\n",
    "                subtext = subtext_row.find('td', class_='subtext')\n",
    "                \n",
    "                if subtext:\n",
    "                    # Get points\n",
    "                    score_span = subtext.find('span', class_='score')\n",
    "                    if score_span:\n",
    "                        # Extract the number from \"123 points\"\n",
    "                        points_text = score_span.text.strip()\n",
    "                        points_match = re.search(r'(\\d+)', points_text)\n",
    "                        if points_match:\n",
    "                            points = int(points_match.group(1))\n",
    "                    \n",
    "                    # Get author\n",
    "                    user_link = subtext.find('a', class_='hnuser')\n",
    "                    if user_link:\n",
    "                        author = user_link.text.strip()\n",
    "                    \n",
    "                    # Get age\n",
    "                    age_span = subtext.find('span', class_='age')\n",
    "                    if age_span:\n",
    "                        age = age_span.text.strip()\n",
    "                    \n",
    "                    # Get comment count\n",
    "                    links = subtext.find_all('a')\n",
    "                    for link in links:\n",
    "                        if 'comment' in link.text or 'discuss' in link.text:\n",
    "                            # Extract the number from \"123 comments\" or default to 0 for \"discuss\"\n",
    "                            comment_text = link.text.strip()\n",
    "                            comment_match = re.search(r'(\\d+)', comment_text)\n",
    "                            if comment_match:\n",
    "                                comments = int(comment_match.group(1))\n",
    "                            break\n",
    "            \n",
    "            # Add the article data to our list\n",
    "            article_data = {\n",
    "                'title': title,\n",
    "                'url': url,\n",
    "                'points': points,\n",
    "                'author': author,\n",
    "                'age': age,\n",
    "                'comments': comments\n",
    "            }\n",
    "            \n",
    "            articles.append(article_data)\n",
    "            \n",
    "            # Break if we've reached the desired number of articles\n",
    "            if len(articles) >= num_titles:\n",
    "                break\n",
    "        \n",
    "        # Move to the next page\n",
    "        page += 1\n",
    "        \n",
    "        # Be nice to the server - add a short delay between requests\n",
    "        time.sleep(1)\n",
    "    \n",
    "    return articles[:num_titles]  # Ensure we return exactly the number requested\n",
    "\n",
    "def save_to_csv(articles):\n",
    "    # Generate a filename with current date and time\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"hackernews_articles_{timestamp}.csv\"\n",
    "    \n",
    "    # Write articles to CSV file\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        # Define fields for the CSV\n",
    "        fieldnames = ['title', 'url', 'points', 'author', 'age', 'comments']\n",
    "        \n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()  # Write header row\n",
    "        \n",
    "        for article in articles:\n",
    "            writer.writerow(article)\n",
    "    \n",
    "    return filename\n",
    "\n",
    "def main():\n",
    "    num_articles = 100\n",
    "    print(f\"Scraping {num_articles} Hacker News articles...\")\n",
    "    \n",
    "    articles = scrape_hackernews(num_articles)\n",
    "    \n",
    "    if articles:\n",
    "        print(f\"Found {len(articles)} articles.\")\n",
    "        filename = save_to_csv(articles)\n",
    "        print(f\"Articles saved to {filename}\")\n",
    "        \n",
    "        # Print a preview of the first 5 articles to verify data\n",
    "        print(\"\\nPreview of the first 5 articles:\")\n",
    "        for i, article in enumerate(articles[:5]):\n",
    "            print(f\"\\nArticle {i+1}:\")\n",
    "            for key, value in article.items():\n",
    "                print(f\"  {key}: {value}\")\n",
    "    else:\n",
    "        print(\"No articles were found or there was an error.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cb85cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.read_csv('hackernews_articles_20250407_110949.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c991708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>points</th>\n",
       "      <th>author</th>\n",
       "      <th>age</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rsync replaced with openrsync on macOS Sequoia</td>\n",
       "      <td>https://derflounder.wordpress.com/2025/04/06/r...</td>\n",
       "      <td>330</td>\n",
       "      <td>zdw</td>\n",
       "      <td>11 hours ago</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AI masters Minecraft: DeepMind program finds d...</td>\n",
       "      <td>https://www.nature.com/articles/d41586-025-010...</td>\n",
       "      <td>61</td>\n",
       "      <td>LinuxBender</td>\n",
       "      <td>5 hours ago</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Glamorous Toolkit</td>\n",
       "      <td>https://gtoolkit.com//</td>\n",
       "      <td>152</td>\n",
       "      <td>radeeyate</td>\n",
       "      <td>9 hours ago</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We asked camera companies why their RAW format...</td>\n",
       "      <td>https://www.theverge.com/tech/640119/camera-ra...</td>\n",
       "      <td>113</td>\n",
       "      <td>Tomte</td>\n",
       "      <td>8 hours ago</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Writing C for Curl</td>\n",
       "      <td>https://daniel.haxx.se/blog/2025/04/07/writing...</td>\n",
       "      <td>46</td>\n",
       "      <td>TangerineDream</td>\n",
       "      <td>2 hours ago</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Database Protocols Are Underwhelming</td>\n",
       "      <td>https://byroot.github.io/performance/2025/03/2...</td>\n",
       "      <td>109</td>\n",
       "      <td>PaulHoule</td>\n",
       "      <td>1 day ago</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Let's Ban Billboards</td>\n",
       "      <td>https://iambateman.com/articles/billboards</td>\n",
       "      <td>318</td>\n",
       "      <td>iambateman</td>\n",
       "      <td>8 hours ago</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Serving Vector Tiles, Fast</td>\n",
       "      <td>https://spatialists.ch/posts/2025/04-05-servin...</td>\n",
       "      <td>98</td>\n",
       "      <td>altilunium</td>\n",
       "      <td>1 day ago</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Show HN: Clawtype v2.1 â€“ a one-hand chorded US...</td>\n",
       "      <td>https://www.youtube.com/watch?v=N2PSiOl-auM</td>\n",
       "      <td>101</td>\n",
       "      <td>akavel</td>\n",
       "      <td>2 days ago</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Emulating an iPhone in QEMU</td>\n",
       "      <td>https://eshard.com/posts/emulating-ios-14-with...</td>\n",
       "      <td>258</td>\n",
       "      <td>walterbell</td>\n",
       "      <td>1 day ago</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0      Rsync replaced with openrsync on macOS Sequoia   \n",
       "1   AI masters Minecraft: DeepMind program finds d...   \n",
       "2                                   Glamorous Toolkit   \n",
       "3   We asked camera companies why their RAW format...   \n",
       "4                                  Writing C for Curl   \n",
       "..                                                ...   \n",
       "95               Database Protocols Are Underwhelming   \n",
       "96                               Let's Ban Billboards   \n",
       "97                         Serving Vector Tiles, Fast   \n",
       "98  Show HN: Clawtype v2.1 â€“ a one-hand chorded US...   \n",
       "99                        Emulating an iPhone in QEMU   \n",
       "\n",
       "                                                  url  points          author  \\\n",
       "0   https://derflounder.wordpress.com/2025/04/06/r...     330             zdw   \n",
       "1   https://www.nature.com/articles/d41586-025-010...      61     LinuxBender   \n",
       "2                              https://gtoolkit.com//     152       radeeyate   \n",
       "3   https://www.theverge.com/tech/640119/camera-ra...     113           Tomte   \n",
       "4   https://daniel.haxx.se/blog/2025/04/07/writing...      46  TangerineDream   \n",
       "..                                                ...     ...             ...   \n",
       "95  https://byroot.github.io/performance/2025/03/2...     109       PaulHoule   \n",
       "96         https://iambateman.com/articles/billboards     318      iambateman   \n",
       "97  https://spatialists.ch/posts/2025/04-05-servin...      98      altilunium   \n",
       "98        https://www.youtube.com/watch?v=N2PSiOl-auM     101          akavel   \n",
       "99  https://eshard.com/posts/emulating-ios-14-with...     258      walterbell   \n",
       "\n",
       "             age  comments  \n",
       "0   11 hours ago       255  \n",
       "1    5 hours ago        30  \n",
       "2    9 hours ago        33  \n",
       "3    8 hours ago        36  \n",
       "4    2 hours ago         4  \n",
       "..           ...       ...  \n",
       "95     1 day ago        29  \n",
       "96   8 hours ago       270  \n",
       "97     1 day ago        17  \n",
       "98    2 days ago        25  \n",
       "99     1 day ago        62  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7547d960",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
